{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0dc740",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "![Screenshot](./flow_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9854404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader #To load the csv file (data containing companys faq)\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint,HuggingFaceEmbeddings # Load the llm and embedding model from huggingface\n",
    "from langchain_chroma import Chroma #Vectorstore to store the embedded vectors\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain # \"Combines a retriever (to fetch docs) with the 'create_stuff_document_chain' to automate end-to-end retrieval + answering.\"\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #\"Formats retrieved documents + question into a prompt and passes it to the LLM for answering.\"\n",
    "from langchain_core.tools import tool # To create a custom tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e66500",
   "metadata": {},
   "source": [
    "## Step-1 Load the data (company_Q1.csv)\n",
    "\n",
    "#### load the CSV file that Contains the FAQ question regarding the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9b5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./company_QA.csv\" #Path to the file\n",
    "loader = CSVLoader(file_path=file_path) #CSVLoader to load the CSV file\n",
    "docs = []\n",
    "for doc in loader.lazy_load(): #Perfom lazy load\n",
    "    docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea4713e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) # Total number of document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd68e33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './company_QA.csv', 'row': 0}, page_content=\"Question: Where is the company's headquarters located?\\nAnswer: Our headquarters is located in San Francisco, California. Nestled in the vibrant downtown area, it provides easy access to public transport and major city landmarks.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0] #first document object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb32fa9",
   "metadata": {},
   "source": [
    "### Note : No need to performing chunking since each document object is a single row of the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ec184",
   "metadata": {},
   "source": [
    "## Step-2 Load the LLM and Embedding model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d67a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Lionel Messi is a world-renowned professional footballer (soccer player) from Argentina. He currently plays for Paris Saint-Germain F.C. in the French Ligue 1 and captains the Argentina national team. Messi is widely regarded as one of the greatest footballers of all time. He spent the majority of his illustrious career at FC Barcelona, where he won numerous titles, including multiple Spanish La Liga and UEFA Champions League titles, as well as several personal awards, including the FIFA World Player of the Year and the Ballon d'Or awards multiple times. Messi is known for his exceptional dribbling, passing, and scoring abilities, making him a dual threat both individually and for the team. His speed, precision, and creativity have been key factors in his success on the pitch.\", additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=172, prompt_tokens=8, total_tokens=180), 'model': '', 'finish_reason': 'stop'}, id='run-523d5b90-1626-4ff2-87da-63701b2e87d9-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Mistral 7b model \n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  #Repo for mistral\n",
    "api_key = \"\" #Api key to access the hugging face(Please use your own API Key)\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = repo_id,\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    temperature = 0.3, \n",
    "    max_new_tokens=200   # Max number of tokens to generate in the final output\n",
    "\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Test the model if working properly\n",
    "model.invoke(\"who is messi ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c4c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the embedding model \n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fab60f",
   "metadata": {},
   "source": [
    "## Step-3 Creating a Vectorstore and a retriever\n",
    "\n",
    "#### a. Chroma vector store to store the embedding vectors\n",
    "#### b. retriever to fetch the relevant documents based on user query from vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6625bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and added documents to vector store\n",
    "db = Chroma.from_documents(docs,  #Document object \n",
    "                            embedding_model) #Huggingface embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b079fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a retriever\n",
    "retriever = db.as_retriever(search_type = \"mmr\"  #Maximux-marginal-relevance \n",
    "                            ,search_kwargs = {'k':2,'lambda_mult':0.4} # 'k': select top 2 similar documents and 'lambda_mult': for diverse documents \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeac6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Document-1\n",
      "Question: Can interns take leave?\n",
      "Answer: Interns accrue 1 day/month after 3 months of service.\n",
      "---Document-2\n",
      "Question: Are there COVID-19 protocols currently in place?\n",
      "Answer: We adhere to the latest health guidelines, including optional mask use, hand sanitation stations throughout the facility, and enhanced cleaning schedules.\n"
     ]
    }
   ],
   "source": [
    "# Test how well is the retriever working \n",
    "query = \"What is the leave policy for the intern ?\"\n",
    "result = retriever.invoke(query)\n",
    "for index , res in enumerate(result):\n",
    "    print(f\"---Document-{index+1}\")\n",
    "    print(res.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47adebf",
   "metadata": {},
   "source": [
    "## Step-4 Creating a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb012ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template = \"\"\"You are a helpful AI assistance.Please answer the given user question only based on the context provided. \n",
    "'context'\n",
    "{context}\n",
    "'user question'\n",
    "{input}\"\"\",\n",
    "input_variables=['context','input']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32778429",
   "metadata": {},
   "source": [
    "## Step-5 Create a Chain \n",
    "\n",
    "#### Creating a RAG_Chain by combining components like 'prompt template' , 'retriever' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f4797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a chain that \"Formats retrieved documents + question into a prompt and passes it to the LLM for answering.\"\n",
    "combine_docs_chain = create_stuff_documents_chain(model, template)\n",
    "# To create a final chain to reterive, format prompt and generate answer \n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47a4e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The leave policy for an intern is that they accrue 1 day/month after 3 months of service.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a query to the llm to reg the final response\n",
    "result = rag_chain.invoke({\n",
    "    'input':query\n",
    "})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ba9f9",
   "metadata": {},
   "source": [
    "## Step-6 Creating tools \n",
    "\n",
    "### Tool A. VectorStore Retriever tool (Convert the rag_chain into a tool)\n",
    "\n",
    "#### Redirect to this tool if the user queries is regarding the companies HR Policy\n",
    "\n",
    "### Tool B. DuckDuckSeach Tool\n",
    "\n",
    "#### Redirect to this tool if the user query is general \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3319bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_vectorstore_tool(query:str)->str:\n",
    "    \"\"\"RAG solution for a companys HR Policy FAQ\"\"\"\n",
    "    return rag_chain.invoke({'input':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7ac633",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "@tool\n",
    "def duckducksearch_tool(query:str)->str:\n",
    "    \"\"\"Perform duckduck search when user query is other then companies HR policy\"\"\"\n",
    "    return search.invoke(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e8805",
   "metadata": {},
   "source": [
    "## Step-7 Bind the llm with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de0d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_vectorstore_tool,duckducksearch_tool]\n",
    "llm_with_tools = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3423d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'retrieve_vectorstore_tool',\n",
       "  'args': {'query': 'leave policy for intern at [Company_name]'},\n",
       "  'id': '0',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73beb14d",
   "metadata": {},
   "source": [
    "## Step-8 Define the langgraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d33d6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated,TypedDict #Annotated for labelling and TypeDict to maintain graph state \n",
    "from langchain_core.messages import AnyMessage #Human message or Ai Message\n",
    "from langgraph.graph.message import add_messages  ## Reducers in Langgraph \n",
    "\n",
    "#Schema info that flows through the nodes\n",
    "class state(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94a31071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode #specialized node designed to execute tools within our workflow.\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from IPython.display import Image, display #to visualize the Graph\n",
    "\n",
    "\n",
    "\n",
    "#Function that decides which tool to use for serving the userquery\n",
    "def tool_calling_llm(state:state) ->state:\n",
    "    return {\"messages\":[llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "# Initialize the StateGraph\n",
    "builder = StateGraph(state_schema=state)\n",
    "\n",
    "#Adding Nodes\n",
    "builder.add_node('tool_calling_llm',tool_calling_llm) #returns the tools that is to be used\n",
    "builder.add_node('tools',ToolNode(tools=tools)) #Uses the tool specified to fetch result\n",
    "\n",
    "#Adding Edges\n",
    "builder.add_edge(START,'tool_calling_llm')\n",
    "builder.add_conditional_edges(\n",
    "    'tool_calling_llm',\n",
    "    # If the latest message from AI is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message from AI is a not a tool call -> tools_condition routes to LLM, then generate final response and END\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge('tools','tool_calling_llm')\n",
    "\n",
    "#Compile the graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff55994a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "422 Client Error: Unprocessable Entity for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions (Request ID: Root=1-680f9292-091791d7299d66712e3d301f;b0f536fb-07d8-4470-86c1-52325d9b4d58)\n\nTemplate error: syntax error: Tool call IDs should be alphanumeric strings with length 9! (in <string>:81)\n{\"error\":\"Template error: syntax error: Tool call IDs should be alphanumeric strings with length 9! (in <string>:81)\",\"error_type\":\"template_error\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 422 Client Error: Unprocessable Entity for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwho is messi ?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2795\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2793\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2794\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2795\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2796\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2799\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2800\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2801\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2804\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2805\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2806\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2433\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2427\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2428\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2429\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2430\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2431\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2432\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtool_calling_llm\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtool_calling_llm\u001b[39m(state:state) ->state:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[43mllm_with_tools\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5416\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5409\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5411\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5414\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5415\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5417\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5419\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5420\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:369\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m     **kwargs: Any,\n\u001b[32m    365\u001b[39m ) -> BaseMessage:\n\u001b[32m    366\u001b[39m     config = ensure_config(config)\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    368\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    379\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:946\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    939\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    943\u001b[39m     **kwargs: Any,\n\u001b[32m    944\u001b[39m ) -> LLMResult:\n\u001b[32m    945\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:765\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    763\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    764\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m         )\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    773\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1011\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1009\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1010\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1015\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:370\u001b[39m, in \u001b[36mChatHuggingFace._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_huggingface_endpoint(\u001b[38;5;28mself\u001b[39m.llm):\n\u001b[32m    369\u001b[39m     message_dicts = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(answer)\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:992\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    964\u001b[39m parameters = {\n\u001b[32m    965\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    966\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    983\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    984\u001b[39m }\n\u001b[32m    985\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    986\u001b[39m     inputs=messages,\n\u001b[32m    987\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    991\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:357\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Prabal Kuinkel\\Desktop\\RAG-Based_QA_System\\berrybytesrag\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 422 Client Error: Unprocessable Entity for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions (Request ID: Root=1-680f9292-091791d7299d66712e3d301f;b0f536fb-07d8-4470-86c1-52325d9b4d58)\n\nTemplate error: syntax error: Tool call IDs should be alphanumeric strings with length 9! (in <string>:81)\n{\"error\":\"Template error: syntax error: Tool call IDs should be alphanumeric strings with length 9! (in <string>:81)\",\"error_type\":\"template_error\"}\n",
      "During task with name 'tool_calling_llm' and id '4381783c-8904-427c-c60c-9686dbb9fecf'"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\n",
    "    'messages':\"who is messi ?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a852f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(query).tool_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berrybytesrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
