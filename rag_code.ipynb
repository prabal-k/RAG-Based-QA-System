{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0dc740",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "![Screenshot](./flow_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9854404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader #To load the csv file (data containing companys faq)\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint,HuggingFaceEmbeddings # Load the llm and embedding model from huggingface\n",
    "from langchain_chroma import Chroma #Vectorstore to store the embedded vectors\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain # \"Combines a retriever (to fetch docs) with the 'create_stuff_document_chain' to automate end-to-end retrieval + answering.\"\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #\"Formats retrieved documents + question into a prompt and passes it to the LLM for answering.\"\n",
    "from langchain_core.tools import tool # To create a custom tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e66500",
   "metadata": {},
   "source": [
    "## Step-1 Load the data (company_Q1.csv)\n",
    "\n",
    "#### load the CSV file that Contains the FAQ question regarding the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9b5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./company_QA.csv\" #Path to the file\n",
    "loader = CSVLoader(file_path=file_path) #CSVLoader to load the CSV file\n",
    "docs = []\n",
    "for doc in loader.lazy_load(): #Perfom lazy load\n",
    "    docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea4713e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) # Total number of document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd68e33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './company_QA.csv', 'row': 0}, page_content=\"Question: Where is the company's headquarters located?\\nAnswer: Our headquarters is located in San Francisco, California. Nestled in the vibrant downtown area, it provides easy access to public transport and major city landmarks.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0] #first document object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb32fa9",
   "metadata": {},
   "source": [
    "### Note : No need to performing chunking since each document object is a single row of the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ec184",
   "metadata": {},
   "source": [
    "## Step-2 Load the LLM and Embedding model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d67a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Lionel Messi is a world-renowned professional footballer from Argentina. He is considered one of the greatest players of all time and has spent the majority of his professional career playing for FC Barcelona in La Liga, where he won numerous accolades, including multiple FIFA World Player of the Year awards and the European Golden Shoe for top goalscorer in Europe. Messi also plays for the Argentina national team. His skills, speed, and precision with the ball have made him an iconic figure in global sports.', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=107, prompt_tokens=8, total_tokens=115), 'model': '', 'finish_reason': 'stop'}, id='run-d76970b0-e299-4b95-9b0b-5319221beafa-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Mistral 7b model \n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  #Repo for mistral\n",
    "api_key = \"\" #Api key to access the hugging face(Please use your own API Key)\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = repo_id,\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    temperature = 0.3, \n",
    "    max_new_tokens=200   # Max number of tokens to generate in the final output\n",
    "\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Test the model if working properly\n",
    "model.invoke(\"who is messi ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c4c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the embedding model \n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fab60f",
   "metadata": {},
   "source": [
    "## Step-3 Creating a Vectorstore and a retriever\n",
    "\n",
    "#### a. Chroma vector store to store the embedding vectors\n",
    "#### b. retriever to fetch the relevant documents based on user query from vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6625bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and added documents to vector store\n",
    "db = Chroma.from_documents(docs,  #Document object \n",
    "                            embedding_model) #Huggingface embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b079fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a retriever\n",
    "retriever = db.as_retriever(search_type = \"mmr\"  #Maximux-marginal-relevance \n",
    "                            ,search_kwargs = {'k':2,'lambda_mult':0.4} # 'k': select top 2 similar documents and 'lambda_mult': for diverse documents \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeac6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Document-1\n",
      "Question: Can interns take leave?\n",
      "Answer: Interns accrue 1 day/month after 3 months of service.\n",
      "---Document-2\n",
      "Question: Are there COVID-19 protocols currently in place?\n",
      "Answer: We adhere to the latest health guidelines, including optional mask use, hand sanitation stations throughout the facility, and enhanced cleaning schedules.\n"
     ]
    }
   ],
   "source": [
    "# Test how well is the retriever working \n",
    "query = \"What is the leave policy for the intern ?\"\n",
    "result = retriever.invoke(query)\n",
    "for index , res in enumerate(result):\n",
    "    print(f\"---Document-{index+1}\")\n",
    "    print(res.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47adebf",
   "metadata": {},
   "source": [
    "## Step-4 Creating a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb012ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template = \"\"\"You are a helpful AI assistance.Please answer the given user question only based on the context provided. \n",
    "'context'\n",
    "{context}\n",
    "'user question'\n",
    "{input}\"\"\",\n",
    "input_variables=['context','input']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32778429",
   "metadata": {},
   "source": [
    "## Step-5 Create a Chain \n",
    "\n",
    "#### Creating a RAG_Chain by combining components like 'prompt template' , 'retriever' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4797f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a chain that \"Formats retrieved documents + question into a prompt and passes it to the LLM for answering.\"\n",
    "combine_docs_chain = create_stuff_documents_chain(model, template)\n",
    "# To create a final chain to reterive, format prompt and generate answer \n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a4e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The leave policy for the intern is that they accrue 1 day of leave per month after completing 3 months of service.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a query to the llm to reg the final response\n",
    "result = rag_chain.invoke({\n",
    "    'input':query\n",
    "})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ba9f9",
   "metadata": {},
   "source": [
    "## Step-6 Creating tools \n",
    "\n",
    "### Tool A. VectorStore Retriever tool (Convert the rag_chain into a tool)\n",
    "\n",
    "#### Redirect to this tool if the user queries is regarding the companies HR Policy\n",
    "\n",
    "### Tool B. DuckDuckSeach Tool\n",
    "\n",
    "#### Redirect to this tool if the user query is general \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3319bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_vectorstore_tool(query:str)->str:\n",
    "    \"\"\"RAG solution for a companys HR Policy FAQ\"\"\"\n",
    "    return rag_chain.invoke({'input':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e7ac633",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "@tool\n",
    "def duckducksearch_tool(query:str)->str:\n",
    "    \"\"\"Perform duckduck search when user query is other then companies HR policy\"\"\"\n",
    "    return search.invoke(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e8805",
   "metadata": {},
   "source": [
    "## Step-7 Bind the llm with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2de0d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_vectorstore_tool,duckducksearch_tool]\n",
    "llm_with_tools = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3423d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'retrieve_vectorstore_tool',\n",
       "  'args': {'query': 'leave policy for intern at [Company_name]'},\n",
       "  'id': '0',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d6917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berrybytesrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
